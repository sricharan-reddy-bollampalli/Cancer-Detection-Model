{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feature_engineering as feat\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.prior_probs = None\n",
    "        self.conditional_probs = None\n",
    "\n",
    "    def calculate_prior(self, y):\n",
    "        classes = np.unique(y)\n",
    "        prior_probs = {}\n",
    "        for label in classes:\n",
    "            prior_probs[label] = np.sum(y == label) / len(y)\n",
    "        return prior_probs\n",
    "    \n",
    "    def calculate_conditional_probabilities(self, X, y, k=1):\n",
    "        features = X.columns\n",
    "        classes = np.unique(y)\n",
    "        conditional_probs = {}\n",
    "        for feature in features:\n",
    "            conditional_probs[feature] = {}\n",
    "            for label in classes:\n",
    "                label_rows = X[y == label]\n",
    "                feature_counts = label_rows[feature].value_counts()\n",
    "                feature_probs = {}\n",
    "                for index, value in feature_counts.items():\n",
    "                    feature_probs[index] = (value + k) / (np.sum(y == label) + k*len(feature_counts))\n",
    "                conditional_probs[feature][label] = feature_probs\n",
    "        return conditional_probs\n",
    "\n",
    "    def predict_class(self, x_new):\n",
    "        posterior_probs = {}\n",
    "        for label in self.prior_probs:\n",
    "            posterior_probs[label] = self.prior_probs[label]\n",
    "            for feature in x_new.index:\n",
    "                value = x_new[feature]\n",
    "                feature_probs = self.conditional_probs[feature][label]\n",
    "                if value in feature_probs:\n",
    "                    posterior_probs[label] *= feature_probs[value]\n",
    "                else:\n",
    "                    posterior_probs[label] *= (1 - sum(feature_probs.values()))\n",
    "        return max(posterior_probs, key=posterior_probs.get)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.prior_probs = self.calculate_prior(y_train)\n",
    "        self.conditional_probs = self.calculate_conditional_probabilities(\n",
    "            X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_preds = []\n",
    "        for i in range(len(X_test)):\n",
    "            x_new = X_test.iloc[i]\n",
    "            y_pred = self.predict_class(x_new)\n",
    "            y_preds.append(y_pred)\n",
    "        return y_preds\n",
    "    \n",
    "    def convert(self,y_give):\n",
    "        for i in range(len(y_give)):\n",
    "            if(y_give[i] == ' >50K'):\n",
    "                y_give[i] = 1\n",
    "            else:\n",
    "                y_give[i] = 0\n",
    "                \n",
    "        return y_give\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        count = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if (y_true[i] == y_pred[i]):\n",
    "                count += 1\n",
    "        return count / len(y_true)\n",
    "    \n",
    "    def precision(self, y_true, y_pred):\n",
    "        \n",
    "            true_positives = 0\n",
    "            false_positives = 0\n",
    "            for i in range(len(y_true)):\n",
    "                if y_pred[i] == 0 and y_true[i] == 1:\n",
    "                    true_positives += 1\n",
    "                elif y_pred[i] == 1 and y_true[i] != 1:\n",
    "                    false_positives += 1\n",
    "            if true_positives + false_positives == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                precision = true_positives / (true_positives + false_positives)\n",
    "            return precision \n",
    "\n",
    "    def recall(self, y_true, y_pred):\n",
    "        true_positives = 0\n",
    "        false_negatives = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_pred[i] == 1 and y_true[i] == 1:\n",
    "                true_positives += 1\n",
    "            elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "                false_negatives += 1\n",
    "        if true_positives + false_negatives == 0:\n",
    "            return 0\n",
    "        return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/varshith/Downloads/adult.csv',\n",
    "                 header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                                     'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                                     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                                     'salary'])\n",
    "\n",
    "df.replace(' ?', value = np.nan , inplace = True)                               \n",
    "fe = feat.FeatureEngineering(df.copy())\n",
    "fe.task1()\n",
    "df = fe.ds\n",
    "df['age'] = pd.cut(df['age'], bins=10, right=False)\n",
    "df['fnlwgt'] = pd.cut(df['fnlwgt'], bins=10, right=False)\n",
    "df['education-num']= pd.cut(df['education-num'], bins=10, right=False)\n",
    "df['capital-gain'] = pd.cut(df['capital-gain'], bins=10, right=False)\n",
    "df['capital-loss'] = pd.cut(df['capital-loss'], bins=10, right=False)\n",
    "df['hours-per-week'] = pd.cut(df['hours-per-week'], bins=10, right=False)\n",
    "\n",
    "\n",
    "df_train = df.sample(frac=0.67, random_state=36)\n",
    "df_test = df.drop(df_train.index)\n",
    "X_train = df_train.loc[:, :\"native-country\"]\n",
    "y_train = df_train.loc[:, \"salary\":\"salary\"]\n",
    "X_test = df_test.loc[:, :\"native-country\"]\n",
    "y_test = df_test.loc[:, \"salary\":\"salary\"]\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.15914378780829\n",
      "precision: 30.046948356807512\n",
      "recall: 77.60497667185071\n",
      "F1_score: 21.66048331852495\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "y_pred = nb.convert(y_pred)\n",
    "y_test = nb.convert(y_test)\n",
    "acc = nb.accuracy(y_test, y_pred)\n",
    "precision = nb.precision(y_test, y_pred)\n",
    "recall = nb.recall(y_test,y_pred)\n",
    "print(\"Accuracy:\", acc*100)\n",
    "print(\"precision:\",precision*100)\n",
    "print(\"recall:\",recall*100)\n",
    "\n",
    "F1_score = 2*recall*precision/(recall+precision)\n",
    "\n",
    "print(\"F1_score:\",F1_score*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
